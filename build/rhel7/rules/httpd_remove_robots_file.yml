conflicts: []
definition_location: /home/chase/_work/complianceascode-content/complianceascode-content/linux_os/guide/services/http/securing_httpd/httpd_secure_content/httpd_remove_robots_file/rule.yml
description: 'Remove any <tt>robots.txt</tt> files that may exist with any web content.

  Other methods must be employed if there is information on the web site that

  needs protection from search engines and public view. Inspect all instances of

  <tt>DocumentRoot</tt> and <tt>Alias</tt> and remove any <tt>robots.txt</tt> file.

  <pre>$ sudo rm -f path/to/robots.txt</pre>'
identifiers: {}
inherited_platforms: []
ocil: 'Inspect all instances of <tt>DocumentRoot</tt> and <tt>Alias</tt>. No

  <tt>robots.txt</tt> file should exist.'
ocil_clause: it is not
oval_external_content: null
platform: null
platforms: !!set {}
prodtype: rhel7,rhel8
rationale: 'Search engines are constantly at work on the Internet. Search engines
  are

  augmented by agents, often referred to as spiders or bots, which endeavor to

  capture and catalog web-site content. In turn, these search engines make the

  content they obtain and catalog available to any public web user.

  <br /><br />

  To request

  that a well behaved search engine not crawl and catalog a site, the web site may

  contain a file called robots.txt. This file contains directories and files that

  the web server SA desires not be crawled or cataloged, but this file can also be

  used, by an attacker or poorly coded search engine, as a directory and file

  index to a site. This information may be used to reduce an attacker''s time

  searching and traversing the web site to find files that might be relevant. If

  information on the web site needs to be protected from search engines and public

  view, other methods must be used.'
references:
  stigid: WG310
requires: []
severity: medium
template: null
title: The robots.txt Files Must Not Exist
warnings: []
